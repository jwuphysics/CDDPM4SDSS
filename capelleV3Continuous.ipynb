{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb7e0d7-a272-4135-9a6a-0424dc87bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this implicitly imports numpy, pandas, etc\n",
    "from fastai.basics import *\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f377e2-efaa-4785-b828-20ebc8d260b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import Unet\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb594ee-a8f3-484c-898e-d582d2034ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DataParallel\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Mac users may need device = 'mps' (untested)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32e504-932d-48b0-9980-018530966c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4208197-56e4-46fe-a504-6c23fc55a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5a897-ffe7-43d5-883c-a7988b73993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4ad6e-e274-4c35-b187-3001822bc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9d519-e514-4b3a-b89e-7552ccffce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "image_size = 64\n",
    "sample_percentage = 10\n",
    "valid_bptclasses = [1, 2, 3]\n",
    "global_num_quantiles = 6\n",
    "\n",
    "tab_df = pd.read_csv(\"agn_cleaned.csv\")\n",
    "\n",
    "tab_df = tab_df[\n",
    "    (tab_df['oh_p50'] > 0) & \n",
    "    (tab_df['lgm_tot_p50'] > 0) &\n",
    "    (tab_df['sfr_tot_p50'] > -10) &\n",
    "    (tab_df['bptclass'].isin(valid_bptclasses))\n",
    "]\n",
    "\n",
    "tab_df = tab_df[:int(sample_percentage / 100 * tab_df.shape[0])]\n",
    "num_samples = tab_df.shape[0]\n",
    "\n",
    "objIDs = tab_df['objID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9491ee-42d8-47b5-a8bf-47c89e2bed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2a6c9-cca2-45b7-b7cf-d2afcc525d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_dataset = np.column_stack((tab_df['z'].values, \n",
    "                               tab_df['lgm_tot_p50'].values, \n",
    "                               tab_df['sfr_tot_p50'].values, \n",
    "                               tab_df['oh_p50'].values))\n",
    "\n",
    "#NOTE: Do not use tab_dataset to extract the object IDs. Instead, use objIDs above.\n",
    "\n",
    "print(tab_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42d4e2-b02f-4b45-bcbe-1b91a49490af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(tab_dataset, axis=0)\n",
    "std = np.std(tab_dataset, axis=0)\n",
    "\n",
    "# Get the lower and upper bounds for each column\n",
    "lower_bound = mean - 4 * std\n",
    "upper_bound = mean + 4 * std\n",
    "\n",
    "# Apply the bounds to each value in the dataset\n",
    "for i in range(tab_dataset.shape[1]):\n",
    "    tab_dataset[:, i] = np.clip(tab_dataset[:, i], lower_bound[i], upper_bound[i])\n",
    "\n",
    "# Normalizing the tab_dataset\n",
    "tab_dataset = (tab_dataset - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a1eca-e322-4df2-b218-5de3bb03b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dataset = np.zeros((num_samples, image_size, image_size, 3))\n",
    "\n",
    "# loading and resizing the images\n",
    "for index in range(num_samples): \n",
    "    objID = objIDs.iloc[index]\n",
    "    image_path = f\"images-sdss/{objID}.jpg\"\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((image_size, image_size))\n",
    "    im_dataset[index] = img\n",
    "\n",
    "# Scale pixel values between 0 and 1\n",
    "#im_dataset = im_dataset / 255.\n",
    "\n",
    "im_dataset = np.array(im_dataset)\n",
    "im_dataset = im_dataset.astype(np.uint8)\n",
    "print(im_dataset[0])\n",
    "\n",
    "im_dataset.shape, tab_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72337f1a-a737-4385-bb13-06296e171429",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = tab_dataset[:, 0]\n",
    "lgm_tot_p50_values = tab_dataset[:, 1]\n",
    "sfr_tot_p50_values = tab_dataset[:, 2]\n",
    "oh_p50_values = tab_dataset[:, 3]\n",
    "\n",
    "#---------------------- PLOTTING CODE --------------------------------\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
    "\n",
    "# z values histogram\n",
    "axs[0, 0].hist(z_values, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "axs[0, 0].set_title('z values')\n",
    "axs[0, 0].set_xlabel('Value')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# lgm_tot_p50 values histogram\n",
    "axs[0, 1].hist(lgm_tot_p50_values, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "axs[0, 1].set_title('lgm_tot_p50 values')\n",
    "axs[0, 1].set_xlabel('Value')\n",
    "axs[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# sfr_tot_p50 values histogram\n",
    "axs[1, 0].hist(sfr_tot_p50_values, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "axs[1, 0].set_title('sfr_tot_p50 values')\n",
    "axs[1, 0].set_xlabel('Value')\n",
    "axs[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# oh_p50 values histogram\n",
    "axs[1, 1].hist(oh_p50_values, bins='auto', color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "axs[1, 1].set_title('oh_p50 values')\n",
    "axs[1, 1].set_xlabel('Value')\n",
    "axs[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f593a-1b58-44ed-b909-078681b879b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average and standard deviation of oh_p50 values\n",
    "average_oh_p50 = np.mean(oh_p50_values)\n",
    "std_oh_p50 = np.std(oh_p50_values)\n",
    "\n",
    "average_oh_p50, std_oh_p50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d18f1-c0e7-42f9-9edf-50255946d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22cd90-4927-489e-9c59-5e3bb0e2d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation split (80%/20%)\n",
    "np.random.seed(42)\n",
    "\n",
    "N = len(im_dataset)\n",
    "indices = np.random.permutation(N)\n",
    "train_idxs = indices[:int(0.8*N)]\n",
    "valid_idxs = indices[int(0.8*N):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e56ee9-6b26-4f08-88e7-7378a192f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # batch size\n",
    "epochs = 100\n",
    "image_display_interval = 20\n",
    "\n",
    "def get_x(i):\n",
    "    return im_dataset[i]\n",
    "\n",
    "def get_y(i):\n",
    "    return tab_dataset[i]\n",
    "\n",
    "# create DataBlock\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, RegressionBlock),\n",
    "    get_x=get_x,\n",
    "    get_y=get_y,\n",
    "    splitter=IndexSplitter(valid_idxs), # use your existing validation set\n",
    "    item_tfms=Resize(image_size),\n",
    "    #batch_tfms=[Normalize.from_stats(0.5, 0.5), *aug_transforms(do_flip=True, flip_vert=True, max_rotate=10.0, max_zoom=1.1, max_lighting=0.2, max_warp=0.2, p_affine=0.75, p_lighting=0.75, xtra_tfms=None)]\n",
    "    batch_tfms=Normalize.from_stats(0.5, 0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4434f-4f20-49d6-9072-82c8223996ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "dls = dblock.dataloaders(range(len(im_dataset)), bs=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaa8782-b7b9-44e5-abf9-4f9d58230471",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = dls.one_batch()\n",
    "xb.max(), xb.min(), xb.mean(), xb.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc4434-817c-444a-bc0b-76eec06a1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14311ac5-ac5e-447a-b00d-b85b9fe99bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDDPMCallback(Callback):\n",
    "    def __init__(self, n_steps, beta_min, beta_max, num_conditioned_properties, targets, cfg_scale=0):\n",
    "        store_attr()\n",
    "        self.tensor_type=TensorImage\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.beta = torch.linspace(self.beta_min, self.beta_max, self.n_steps).to(self.dls.device) # variance schedule, linearly increased with timestep\n",
    "        self.alpha = 1. - self.beta \n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        self.sigma = torch.sqrt(self.beta)\n",
    "    \n",
    "    def sample_timesteps(self, x, dtype=torch.long):\n",
    "        return torch.randint(self.n_steps, (x.shape[0],), device=x.device, dtype=dtype)\n",
    "    \n",
    "    def generate_noise(self, x):\n",
    "        return self.tensor_type(torch.randn_like(x))\n",
    "    \n",
    "    def noise_image(self, x, eps, t):\n",
    "        alpha_bar_t = self.alpha_bar[t][:, None, None, None]\n",
    "        return torch.sqrt(alpha_bar_t)*x + torch.sqrt(1-alpha_bar_t)*eps # noisify the image\n",
    "    \n",
    "    def before_batch_training(self):\n",
    "        x0 = self.xb[0] # original images and labels\n",
    "        y0 =  self.yb[0] if np.random.random() > 0.1 else None\n",
    "        \n",
    "        # y0 = None\n",
    "        \n",
    "        eps = self.generate_noise(x0) # noise same shape as x0\n",
    "        t =  self.sample_timesteps(x0) # select random timesteps\n",
    "        xt =  self.noise_image(x0, eps, t)  # add noise to the image\n",
    "        # print(x0.shape, y0.shape, t.shape, xt.shape, eps.shape)\n",
    "        \n",
    "        self.learn.xb = (xt, t, y0) # input to our model is noisy image, timestep and label\n",
    "        self.learn.yb = (eps,) # ground truth is the noise \n",
    "\n",
    "    def sampling_algo(self, xt, t, train_targets=None):\n",
    "        t_batch = torch.full((xt.shape[0],), t, device=xt.device, dtype=torch.long)\n",
    "        z = self.generate_noise(xt) if t > 0 else torch.zeros_like(xt)\n",
    "        alpha_t = self.alpha[t] # get noise level at current timestep\n",
    "        alpha_bar_t = self.alpha_bar[t]\n",
    "        sigma_t = self.sigma[t]\n",
    "        alpha_bar_t_1 = self.alpha_bar[t-1]  if t > 0 else torch.tensor(1, device=xt.device)\n",
    "        beta_bar_t = 1 - alpha_bar_t\n",
    "        beta_bar_t_1 = 1 - alpha_bar_t_1\n",
    "        predicted_noise = self.model(xt, t_batch, targets=train_targets)\n",
    "        if self.cfg_scale>0:\n",
    "            uncond_predicted_noise = self.model(xt, t_batch, targets=None)\n",
    "            predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, self.cfg_scale)\n",
    "        x0hat = (xt - torch.sqrt(beta_bar_t) * predicted_noise)/torch.sqrt(alpha_bar_t)\n",
    "        x0hat = torch.clamp(x0hat, -1, 1)\n",
    "        xt = x0hat * torch.sqrt(alpha_bar_t_1)*(1-alpha_t)/beta_bar_t + xt * torch.sqrt(alpha_t)*beta_bar_t_1/beta_bar_t + sigma_t*z \n",
    "\n",
    "        return xt\n",
    "    \n",
    "    # def sampling_algo_old(self, xt, t, label=None):\n",
    "    #     t_batch = torch.full((xt.shape[0],), t, device=xt.device, dtype=torch.long)\n",
    "    #     z = self.generate_noise(xt) if t > 0 else torch.zeros_like(xt)\n",
    "    #     alpha_t = self.alpha[t] # get noise level at current timestep\n",
    "    #     alpha_bar_t = self.alpha_bar[t]\n",
    "    #     sigma_t = self.sigma[t]\n",
    "    #     xt = 1/torch.sqrt(alpha_t) * (xt - (1-alpha_t)/torch.sqrt(1-alpha_bar_t) * self.model(xt, t_batch, label=label)) + sigma_t*z \n",
    "    #          1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "    #     # predict x_(t-1) in accordance to Algorithm 2 in paper\n",
    "    #     return xt\n",
    "    \n",
    "    def sample(self):\n",
    "        # Randomly generate batch_size property tuples here\n",
    "        xt = self.generate_noise(self.xb[0]) # a full batch at once! \n",
    "        self.targets = torch.randn(xt.shape[0], self.num_conditioned_properties)\n",
    "        # sort by metallicity\n",
    "        self.targets = self.targets[self.targets[:,3].argsort()]\n",
    "        for t in progress_bar(reversed(range(self.n_steps)), total=self.n_steps, leave=False):\n",
    "            xt = self.sampling_algo(xt, t, self.targets) \n",
    "        return xt\n",
    "    \n",
    "    def before_batch_sampling(self):\n",
    "        xt = self.sample()\n",
    "        self.learn.pred = (xt,)\n",
    "        raise CancelBatchException\n",
    "    \n",
    "    def after_validate(self):\n",
    "        if (self.epoch+1) % image_display_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                xt = self.sample()\n",
    "                wandb.log({\"preds\": [wandb.Image(torch.tensor(im)) for im in xt[0:36]]})\n",
    "    \n",
    "    def before_batch(self):\n",
    "        if not hasattr(self, 'gather_preds'): self.before_batch_training()\n",
    "        else: self.before_batch_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683314b2-10fe-468e-b8a4-f1d42b9cb21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA(Callback):\n",
    "    \"Exponential Moving average CB\"\n",
    "    def __init__(self, beta=0.995, pct_start=0.3):\n",
    "        store_attr()\n",
    "        \n",
    "    \n",
    "    def before_fit(self):\n",
    "        self.ema_model = deepcopy(self.model).eval().requires_grad_(False)\n",
    "        self.step_start_ema = int(self.pct_start*self.n_epoch)  #start EMA at 30% of epochs\n",
    "        \n",
    "    def update_model_average(self):\n",
    "        for current_params, ma_params in zip(self.model.parameters(), self.ema_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "    def step_ema(self):\n",
    "        if self.epoch < self.step_start_ema:\n",
    "            self.reset_parameters()\n",
    "            self.step += 1\n",
    "            return\n",
    "        self.update_model_average()\n",
    "        self.step += 1\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.ema_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if hasattr(self, 'pred'): return\n",
    "        self.step_ema()\n",
    "    \n",
    "    def after_training(self):\n",
    "        self.model = self.ema_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf9265-a7f2-41e7-a86b-02b00a3b06b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates(Unet)\n",
    "class ConditionalUnet(Unet):\n",
    "    def __init__(self, dim, num_conditioned_properties=None, **kwargs):\n",
    "        super().__init__(dim=dim, **kwargs)\n",
    "        if num_conditioned_properties is not None:\n",
    "            self.target_MLP = nn.Linear(num_conditioned_properties, dim * 4)\n",
    "    \n",
    "    def forward(self, x, time, targets=None):\n",
    "        x = self.init_conv(x)\n",
    "        t = self.time_mlp(time)\n",
    "        if targets is not None:\n",
    "            out = self.target_MLP(targets)\n",
    "            t += out\n",
    "            \n",
    "        return super().forward_blocks(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918eac7-8585-4ccf-b515-818c299b7302",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConditionalUnet(dim=128, channels=3, num_conditioned_properties=4)\n",
    "model.to(device);\n",
    "model = DataParallel(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca23b0-fd65-4723-a92b-356b60b00058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from fastai.callback.wandb import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797e02c-11f5-49ea-8715-4ec4c8de0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 1000\n",
    "\n",
    "ddpm_learner = Learner(dls, model, \n",
    "                       cbs=[ConditionalDDPMCallback(n_steps=num_timesteps, beta_min=0.0001, beta_max=0.02, num_conditioned_properties=4, targets=tab_dataset, cfg_scale=3),\n",
    "                            EMA()], \n",
    "                       #If the above breaks, change targets\n",
    "                       loss_func=nn.L1Loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e6804-7509-41b9-a8dc-a9f1e8b1fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm_learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7c406-1ba4-47bc-84ee-fb082a9a6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"cond_ddpm_sdss\", group=\"chesapeake_ml\", tags=[\"ddpm\", \"ema\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952a062-e392-4651-aafd-069f9446dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm_learner.fit_one_cycle(epochs, 1e-4, cbs =[SaveModelCallback(monitor=\"train_loss\", fname=\"cond_ddpm_sdss\"), \n",
    "                                           WandbCallback(log_preds=False, log_model=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde8f5c-e2fb-485b-aabe-bc9a81cbd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm_learner.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b701a-74aa-41dd-9dd6-350ca77956b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ddpm_learner.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb9ab9-5239-4c87-9eb4-b6915776afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.Image(torch.tensor(0.5*preds[0][0]+0.5)).image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282bc0a3-58e1-498b-9cc5-e57ee4b2a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2e025-4735-4289-94c1-a162fb1a8613",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44328a0d-5548-4564-a242-6533b2b7c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.mean(dim=(0,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7ff2a-f7ef-4a11-8bc7-66c29654b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 5\n",
    "ncols = int(math.ceil(25/10))\n",
    "axs = subplots(nrows, 10)[1].flat\n",
    "for i, (pred, ax) in enumerate(zip(preds[0], axs)): \n",
    "    ((pred+1)/2).show(ax=ax, title=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f881a12-8107-4c3e-ad8d-a555e9f981cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
